{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"producer": "macOS Version 15.6.1 (Build 24G90) Quartz PDFContext", "creator": "PScript5.dll Version 5.2.2", "creationdate": "D:20250906194222Z00'00'", "title": "Microsoft Word - Basic Ratemaking_Version 5_May 2016_2.docx", "author": "KAREN345", "moddate": "D:20250906194222Z00'00'", "source": "/home/hugo/code/aria/assets/actuarial/5_Werner_Modlin_stripped_EX_appendices.pdf", "total_pages": 313, "page": 165, "page_label": "166", "pdf_path": "/home/hugo/code/aria/assets/actuarial/5_Werner_Modlin_stripped_EX_appendices.pdf", "source_name": "Werner Entire Text"}, "page_content": "costs, GLM analysis is typically performed on loss cost data (or preferably frequency and severity \nseparately).  There are statistical and practical reasons supporting this practice: \n\uf0b7 Modeling loss ratios requires premiums to be adjusted to current rate level at the granular level \nand that can be practically difficult.   \n\uf0b7 Experienced actuaries have an a priori expectation of frequency and severity patterns (e.g., \nyouthful drivers have higher frequencies).  In contrast, the loss ratio patterns are dependent on \nthe current rates.  Thus, the actuary can better distinguish the signal from the noise when building \nmodels. \n\uf0b7 Loss ratio models become obsolete when rates and rating structures are changed. \n\uf0b7 There is no commonly accepted distribution for modeling loss ratios. \nMore details can be found about this in \u201cGLM Basic Modeling: Avoiding Predictive Modeling Pitfalls\u201d \n(Werner and Guven 2007, pp. 263-264).  Best practice also dictates that modeling be performed on a \nhomogeneous body of claims.  For example, personal automobile models are generally performed at the \n                                                      \n33 The Tweedie family of distributions is considered a special extension of the exponential family.", "type": "Document"}}